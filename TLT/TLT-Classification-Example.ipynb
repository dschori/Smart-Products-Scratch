{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Products Classification example usecase\n",
    "\n",
    "This notebook shows an example use case for classification using the Transfer Learning Toolkit.\n",
    "\n",
    "0. [Set up env variables](#head-0)\n",
    "1. [Prepare dataset and pretrained model](#head-1)\n",
    "    1. [Split the dataset into train/test/val and do augmentation](#head-1-1)\n",
    "    2. [Download pre-trained model](#head-1-2)\n",
    "2. [Provide training specfication](#head-2)\n",
    "3. [Run TLT training](#head-3)\n",
    "4. [Evaluate trained models](#head-4)\n",
    "5. [Visualize inferences](#head-5)\n",
    "6. [Export and Deploy!](#head-6)\n",
    "    1. [Int8 Optimization](#head-6-1)\n",
    "    2. [Generate TensorRT engine](#head-6-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup env variables <a class=\"anchor\" id=\"head-0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env USER_EXPERIMENT_DIR=/workspace/tlt-experiments/DO_NOT_DELETE\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tlt-experiments/DO_NOT_DELETE/data\n",
    "%env SPECS_DIR=/workspace/tlt-experiments/DO_NOT_DELETE/specs\n",
    "%env ZIP_DIR=/workspace/tlt-experiments\n",
    "%env KEY=YOUR_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare datasets and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete existing Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /workspace/tlt-experiments/DO_NOT_DELETE/data\n",
    "!rm -rf /workspace/tlt-experiments/DO_NOT_DELETE/classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the SmartProducts Dataset! :)  \n",
    "The Dataset has to be at: **/workspace/tlt-experiments/data/SmartProducts.zip** (from inside Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classes you want to use for Classification.  \n",
    "**NOTE: Only take classes which consists one and only one object per image!**  \n",
    "With the classes below, you should reach accuracy > **95%!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['kellogs_variety', 'gruenespargeln', 'kraeutershampoo', 'kamillentee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Split the dataset into train/test/val and do augmentation <a class=\"anchor\" id=\"head-1-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import random\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from pathlib import Path\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class ImageHandler():\n",
    "    def __init__(self, do_augmentation):\n",
    "        self.image_paths = {}\n",
    "        self.split_image_paths = {}\n",
    "        self.data_archive = None\n",
    "        self.do_augmentation = do_augmentation\n",
    "        self.datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                                          vertical_flip=True,\n",
    "                                          width_shift_range=0.3,\n",
    "                                          height_shift_range=0.3)\n",
    "        \n",
    "    def process(self, save_path):\n",
    "        for class_name in tqdm_notebook(list(self.image_paths.keys()), desc='Classes'):\n",
    "            for split in tqdm_notebook(['train', 'val', 'test'], desc='Splits', leave=False):\n",
    "                for image_file in tqdm_notebook(self.split_image_paths[class_name][split], desc='Files', leave=False):\n",
    "                    img = self.__load_image(image_path=image_file)\n",
    "                    img = self.__allign_horizontal(img=img)\n",
    "                    img = self.__resize(img=img)\n",
    "                    if split == 'train' and self.do_augmentation:\n",
    "                        sample = np.expand_dims(img, 0)\n",
    "                        iterater = self.datagen.flow(sample, batch_size=1)\n",
    "                        for idx in range(6):\n",
    "                            img = iterater.next().squeeze()\n",
    "                            img = img.astype('uint8')\n",
    "                            self.__save_image('{}/{}/{}'.format(save_path, split, image_file), img, idx)\n",
    "                    else:\n",
    "                        self.__save_image('{}/{}/{}'.format(save_path, split, image_file), img)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        for class_name in CLASSES:\n",
    "            random.shuffle(self.image_paths[class_name])\n",
    "    \n",
    "    def add_data_archive(self, path):\n",
    "        self.data_archive = zipfile.ZipFile(path, 'r')\n",
    "        \n",
    "    def add_image(self, class_name, image):\n",
    "        if class_name not in self.image_paths:\n",
    "            self.image_paths[class_name] = []\n",
    "        self.image_paths[class_name].append(image)\n",
    "        \n",
    "    def __load_image(self, image_path):\n",
    "        return self.__bytes_to_numpy(img_as_bytes=self.data_archive.read(image_path))\n",
    "    \n",
    "    def __bytes_to_numpy(self, img_as_bytes):\n",
    "        return cv2.imdecode(np.frombuffer(img_as_bytes, np.uint8), 1)\n",
    "    \n",
    "    def __allign_horizontal(self, img):\n",
    "        if img.shape[0] > img.shape[1]:\n",
    "            return cv2.rotate(img, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "        else:\n",
    "            return img\n",
    "            \n",
    "    def __resize(self, img):\n",
    "        return cv2.resize(img, (320, 240), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "    def __save_image(self, path, img, idx=0):\n",
    "        Path(os.path.split(path)[0]).mkdir(parents=True, exist_ok=True)\n",
    "        cv2.imwrite('{}_{}.{}'.format(path[:-4], idx, 'jpg'), img) \n",
    "        \n",
    "    def split_images(self, train_size=0.6, val_size=0.2):\n",
    "        for class_name, image_list in self.image_paths.items():\n",
    "            if class_name not in self.split_image_paths:\n",
    "                self.split_image_paths[class_name] = {}\n",
    "            self.split_image_paths[class_name]['train'] = image_list[:int(train_size*len(self.image_paths[class_name]))]\n",
    "            self.split_image_paths[class_name]['val'] = image_list[int(train_size*len(self.image_paths[class_name])):int((train_size + val_size)*len(self.image_paths[class_name]))]\n",
    "            self.split_image_paths[class_name]['test'] = image_list[int((train_size + val_size)*len(self.image_paths[class_name])):]\n",
    "\n",
    "image_handler = ImageHandler(do_augmentation=True)\n",
    "\n",
    "for file_path in zipfile.ZipFile('{}/SmartProducts.zip'.format(os.environ.get('ZIP_DIR')), 'r').namelist():\n",
    "    tmp_image_list = []\n",
    "    class_name, file_name = os.path.split(file_path)\n",
    "    if class_name in CLASSES and not file_name.endswith('.txt'):\n",
    "        tmp_image_list.append(file_name)\n",
    "        image_handler.add_image(class_name=class_name, image=file_path)\n",
    "\n",
    "image_handler.add_data_archive(path='{}/SmartProducts.zip'.format(os.environ.get('ZIP_DIR')))\n",
    "image_handler.shuffle()\n",
    "image_handler.split_images(train_size=0.6, val_size=0.2)\n",
    "print('Splitting and Augmenting Files...')\n",
    "image_handler.process('{}/split'.format(os.environ.get('DATA_DOWNLOAD_DIR')))\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def show_images(split, n=4):\n",
    "    rows, columns = n, len(CLASSES)\n",
    "\n",
    "    fig, axis = plt.subplots(rows, columns, figsize=(columns*5, rows*3))\n",
    "\n",
    "    for c, cl in enumerate(CLASSES):\n",
    "        images = os.listdir('./data/split/{}/{}/'.format(split, cl))\n",
    "        random.shuffle(images)\n",
    "        images = images[:rows]\n",
    "        for j in range(rows):\n",
    "            img = load_image(path='./data/split/{}/{}/{}'.format(split, cl, images[j]))\n",
    "            axis[j, c].imshow(img)\n",
    "            axis[j, c].axis('off')\n",
    "            axis[j, c].set_title(cl, fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(split='train', n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(split='val', n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(split='test', n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Download pretrained models <a class=\"anchor\" id=\"head-1-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tlt_pretrained_classification:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $USER_EXPERIMENT_DIR/classification/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tlt_pretrained_classification:resnet18 --dest $USER_EXPERIMENT_DIR/classification/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $USER_EXPERIMENT_DIR/classification/pretrained_resnet18/tlt_pretrained_classification_vresnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provide training specfication <a class=\"anchor\" id=\"head-2\"></a>\n",
    "* Training dataset\n",
    "* Validation dataset\n",
    "* Pre-trained models\n",
    "* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $SPECS_DIR/classification_spec.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run TLT training <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Provide the sample spec file and the output directory location for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tlt-train classification -e $SPECS_DIR/classification_spec.cfg -r $USER_EXPERIMENT_DIR/classification/output -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate trained models <a class=\"anchor\" id=\"head-4\"></a>\n",
    "\n",
    "In this step, we assume that the training is complete and the model from the final epoch (`resnet_005.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_spec.cfg` to point to the intended model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-evaluate classification -e $SPECS_DIR/classification_spec.cfg -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Inferences <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the output results of our model on test images, we can use the `tlt-infer` tool. Note that using models trained for higher epochs will usually result in better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env EPOCH=005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference in directory mode to run on a set of test images.  \n",
    "**Choose from a Class which you specified at the beginning: For Example \"kellogs_variety\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLASS_TO_TEST=kamillentee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-infer classification -m $USER_EXPERIMENT_DIR/classification/output/weights/resnet_$EPOCH.tlt \\\n",
    "                          -k $KEY -b 32 -d $DATA_DOWNLOAD_DIR/split/test/$CLASS_TO_TEST \\\n",
    "                          -cm $USER_EXPERIMENT_DIR/classification/output/classmap.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in Getting Started Guide, this outputs a results.csv file in the same directory. We can use a simple python program to see the visualize the output of csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "import os\n",
    "import csv\n",
    "from math import ceil\n",
    "\n",
    "DATA_DIR = os.environ.get('DATA_DOWNLOAD_DIR')\n",
    "CLASS_TO_TEST = os.environ.get('CLASS_TO_TEST')\n",
    "csv_path = os.path.join(DATA_DIR, 'split', 'test', CLASS_TO_TEST, 'result.csv')\n",
    "results = []\n",
    "with open(csv_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        results.append((row[0], row[1]))\n",
    "        \n",
    "columns = 5\n",
    "rows = 3\n",
    "fig = plt.figure(figsize=(columns*6, rows*6))\n",
    "random.shuffle(results)\n",
    "for i in range(1, columns*rows + 1):\n",
    "    ax = fig.add_subplot(rows, columns,i)\n",
    "    img = Image.open(results[i][0])\n",
    "    plt.imshow(img)\n",
    "    color = 'black'\n",
    "    if CLASS_TO_TEST != results[i][1]:\n",
    "        color = 'red'\n",
    "    ax.set_title('Truth: {}\\n Prediction: {}'.format(CLASS_TO_TEST, results[i][1]), fontsize=28, color=color)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export and Deploy! <a class=\"anchor\" id=\"head-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-export classification \\\n",
    "            -m $USER_EXPERIMENT_DIR/classification/output/weights/resnet_$EPOCH.tlt \\\n",
    "            -o $USER_EXPERIMENT_DIR/classification/export/final_model.etlt \\\n",
    "            -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported model:')\n",
    "print('------------')\n",
    "!ls -lh $USER_EXPERIMENT_DIR/classification/export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Int8 Optimization <a class=\"anchor\" id=\"head-6-1\"></a>\n",
    "Classification model supports int8 optimization for inference in TRT. Inorder to use this, we must calibrate the model to run 8-bit inferences. This involves 2 steps\n",
    "\n",
    "* Generate calibration tensorfile from the training data using tlt-int8-tensorfile\n",
    "* Use tlt-export to generate int8 calibration table.\n",
    "\n",
    "*Note: For this example, we generate a calibration tensorfile containing 10 batches of training data.\n",
    "Ideally, it is best to use atleast 10-20% of the training data to calibrate the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-int8-tensorfile classification -e $SPECS_DIR/classification_spec.cfg \\\n",
    "                                    -m 10 \\\n",
    "                                    -o $USER_EXPERIMENT_DIR/classification/export/calibration.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the pre-existing exported .etlt file.\n",
    "!rm -rf $USER_EXPERIMENT_DIR/classification/export/final_model.etlt\n",
    "!tlt-export classification \\\n",
    "            -m $USER_EXPERIMENT_DIR/classification/output/weights/resnet_$EPOCH.tlt \\\n",
    "            -o $USER_EXPERIMENT_DIR/classification/export/final_model.etlt \\\n",
    "            -k $KEY \\\n",
    "            --cal_data_file $USER_EXPERIMENT_DIR/classification/export/calibration.tensor \\\n",
    "            --data_type int8 \\\n",
    "            --batches 10 \\\n",
    "            --cal_cache_file $USER_EXPERIMENT_DIR/classification/export/final_model_int8_cache.bin \\\n",
    "            -v "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Generate TensorRT engine <a class=\"anchor\" id=\"head-6-2\"></a>\n",
    "Verify engine generation using the `tlt-converter` utility included with the docker.\n",
    "\n",
    "The `tlt-converter` produces optimized tensorrt engines for the platform that it resides on. Therefore, to get maximum performance, please instantiate this docker and execute the `tlt-converter` command, with the exported `.etlt` file and calibration cache (for int8 mode) on your target device. The converter utility included in this docker only works for x86 devices, with discrete NVIDIA GPU's. \n",
    "\n",
    "For the jetson devices, please download the converter for jetson from the dev zone link [here](https://developer.nvidia.com/tlt-converter). \n",
    "\n",
    "If you choose to integrate your model into deepstream directly, you may do so by simply copying the exported `.etlt` file along with the calibration cache to the target device and updating the spec file that configures the `gst-nvinfer` element to point to this newly exported model. Usually this file is called `config_infer_primary.txt` for detection models and `config_infer_secondary_*.txt` for classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-converter $USER_EXPERIMENT_DIR/classification/export/final_model.etlt \\\n",
    "               -k $KEY \\\n",
    "               -c $USER_EXPERIMENT_DIR/classification/export/final_model_int8_cache.bin \\\n",
    "               -o predictions/Softmax \\\n",
    "               -d 3,320,240 \\\n",
    "               -i nchw \\\n",
    "               -m 64 -t int8 \\\n",
    "               -e $USER_EXPERIMENT_DIR/classification/export/final_model.trt \\\n",
    "               -b 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
